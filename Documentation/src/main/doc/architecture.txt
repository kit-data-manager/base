Architecture Overview
---------------------
In the following chapters basic components and concepts of KIT Data Manager's architecture are described briefly. Basically, the presented components are responsible for 
abstracting from underlaying infrastructure and for providing a common service layer for integrating community-specific respository solutions.

[[FigureArchitecture]]
.Basic architecture of KIT Data Manager.
image::Architecture.png[Architecture, width="600px", align="center"]

The <<FigureArchitecture,figure above>> shows a quite general view on the differnt layers and services provided by KIT Data Manager. The following table gives a short impression 
about the resposibilities of each service and in which chapter(s) the services are explained in detail.

[cols="m,n,o", options="header"]
|============================================================================================================================
|Service|Covered in chapter...|Short Description
|Data Sharing|<<ChapterAuthorization,Authorization>>|Authorizing access to data and functionality based on users, groups and roles.
|Metadata Management|<<ChapterMetadataManagement,Metadata Management>>|Managing and accessing different kinds of metadata.
|Search|<<ChapterMetadataManagement,Metadata Management>>,<<ChapterDataOrganization,Data Organization>>|Search for metadata.
|Staging|<<ChapterStaging,Staging>>, <<ChapterDataOrganization,Data Organization>>|Accessing and managing (file-based) data.
|Data Processing|<<ChapterDataWorkflow,Data Workflow>>|Processing of data stored in a KIT Data Manager repository.
|Audit|<<ChapterAudit,Audit>>|Capturing of audit information for repository resources.
|============================================================================================================================

Before describing each service let's take a short look at a core concept of KIT Data Manager: the Digital Object. To be able to cope with the tasks of a repository system (e.g. content preservation,
bit preservation, curation...) there is the need to think in structured, `Digital Object`-based rather than in unstructured, File-based dimensions. Of course, in many cases there 
are files managed by KIT Data Manager on the lowest layer (see <<ChapterDataOrganization,Data Organization>>) but they are only one possible representation of the content of a Digital Object. 
The Digital Object and everything related to it is described by a huge amount of metadata (see <<ChapterMetadataManagement,Metadata Management>>) in order to enable software systems to retrieve and interpret Digital Objects, 
their content and their provenance. However, while reading the following chapters bear always in mind that the core element of KIT Data Manager is the Digital Objects consisting of metadata and data.

[[ChapterAuthorization]]
Authorization
~~~~~~~~~~~~~
One of the core services, which concerns almost every part of KIT Data Manager, is the Authorization. It is based on users, groups and an effective role and determines if the 
access to a secured resource (e.g. a Digital Objects) or operation (e.g. adding users to a group) is granted or not. Authorization decisions in KIT Data Manager are always based on a 
specific context used to issue the request. This context consists of:

UserId::
   An internal user identifier. This identifier is unique for each user and is assigned during registration, e.g. via a Community-specific Web frontend. 
   For real world applications, the user identifier should be obtained from another, central source like an LDAP database in order 
   to provide a common source of authentification for data and metadata access.
GroupId::
   An internal group id. This identifier is unique for each user group of a KIT Data Manager instance. Available groups can be managed by higher level services. 
   By default, there is only one group with the id `USERS` which contains all registered users.

The final part needed for authorization decisions is the `Role`. A role defines, together with UserId and GroupId, the `AuthorizationContext` that is used to access 
resources or functionalities. Currently, there are the following roles available:

[cols="m,n", options="header"]
|============================================================================================================================
|Role|Description
|NO_ACCESS|An AuthorizationContext with this role has no access to any operation or resource.
|MEMBERSHIP_REQUESTED|This is an intermediate role used in special cases. 
 Users with this role have requested the membership for e.g. a group but are not activated, yet.
|GUEST|Using an AuthorizationContext with the role `GUEST` grants read access to public accessible resources. Modifications of resources are not allowed.
|MEMBER|An AuthorizationContext with this role can be used to read and create resources. This role should be used in most cases.
|MANAGER|An AuthorizationContext with the role `MANAGER` can be used for operations that require special access to resources and 
functionalities on a group level, e.g. assign new memberships to a group.
|CURATOR|The CURATOR role is intended to be used for curation tasks, e.g. updating metadata, data formats or removing entities. Currently, the CURATOR role 
is not used explicitly and update can be performed by everybody posessing at least the role MEMBER. For future versions such operations will be reserved for
the CURATOR role in order to avoid uncontrolled modification of repository content.
|ADMINISTRATOR|The role `ADMINISTRATOR` is used for operations that require special access on a global level. Typically, the 
administrator role should be used sparingly and only for a small amount of administrator users.
|============================================================================================================================

Out of these roles, each user has a maximum role `MAX_ROLE` which is defined globally and cannot be bypassed.

[NOTE]
The MAX_ROLE defines the highest role a user may possess. This role might be further restricted on group- or resource-level, so that a maximum role of MANAGER may result in a 
group role MEMBER. On the other hand it is not possible to gain a higher role than MAX_ROLE, which means, if the maximum role of a user is set to NO_ACCESS, the user is not 
allowed to do anything and won't be able to gain more permissions on group- or resource-level. 

To determine the actual role within a specific context, a user has a group-specific role and the MAX_ROLE. The effective role is the group role unless MAX_ROLE is smaller. 
In this case MAX_ROLE is the effective role.

Additionally, there can be resource-specific roles issued separately. By default, each resource with restricted access can be accessed (read and modify) by all 
members of the group the resource has been created in. For sharing purposes it is also possible to issue additional permissions for another groups (called `ResourceReferences`) 
or to single users (called `Grants`). This allows a user who is not a member of the creating group to access a single resource with an assigned role.

Apart from access to resources also access to operations can be restricted. For basic services and components of KIT Data Manager the restriction for operations is 
roughly oriented on the role definition presented above. This means, that the role GUEST is required for read operations, MEMBER is required for basic write operations
and the MANAGER role entitles the caller to remove data (to a very limited extent).

[[ChapterMetadataManagement]]
Metadata Management 
~~~~~~~~~~~~~~~~~~~
The core of the metadata management of KIT Data Manager is the metadata model, which is presented in the <<MetadataModel,figure below>>. 

[[MetadataModel]]
.Metadata model of KIT Data Manager. An important aspect of this model is covered by the Digital Object which provides the Digital Object ID (OID). This OID is used to refer to the according Digital Object by other linked entities, e.g. Staging or Data Organization Metadata.
image::MetaData.png[Metadata model, width="500px", align="center"]

The metadata model constists of three parts which can be described as follows:

Administrative Metadata::
   This category contains metadata elements that are mostly used internally by KIT Data Manager and its services. These elements have a fixed schema and are typically stored 
   in a relational database. One important part is the Base Metadata defining a guaranteed core set of metadata elements that are expected to be available for each and every 
   Digital Object managed by KIT Data Manager. Parts of the Base Metadata are adopted from the link:http://epubs.stfc.ac.uk/bitstream/485/csmdm.version-2.pdf[Core Scientific 
   Metadata Model (CSMD) 2.0], other parts of this model were skipped to reduce complexity and to allow more flexibility. An overview of all Base Metadata entities and how they 
   relate to each other is depicted in figure <<BaseMetadata,BaseMetadata>>. One can see the main entities 'Study', 'Investigation' and 'Digital Object' which also contains 
   the (Digital) Object Identifier also referenced as OID in this document. The OID identifies each Digital Object and can be used to link additional metadata entities to a 
   Digital Object, e.g. Staging Metadata or Sharing Metadata as shown in figure <<MetadataModel,MetadataModel>>. All administrative metadata elements may or may not be exposed 
   to the user by according service interfaces.
Data Organization Metadata::
   The Data Organization Metadata contains information on how data managed by KIT Data Manager is organized/structured and where it is located. Currently, only file-based 
   Data Organization is supported and all Data Organization information is stored in relational databases. For more information please refer to chapter 
   <<ChapterDataOrganization,Data Organization>>.
Content Metadata::
   The third part of the metadata model is the content metadata. Content metadata covers for example community-specific metadata providing detailed information about the 
   content of a Digital Object. For the sake of flexibility all content metadata related implementations are outsourced into a separate module called Enhanced Metadata Module 
   (see <<ChapterEnhancedMetadataArchitecture,Extension: Enhanced Metadata Module>>) that can be installed optionally to the basic KIT Data Manager distribution.
  
[[BaseMetadata]]
.The most important Base Metadata entities and their relations. The core entities, namely Study, Investigation and Digital Object, are highlighted. Furthermore, some of the relationships are simplified for reasons of clarity.
image::BaseMetadata.png[BaseMetadata, width="700px", align="center"]

Administrative and Data Organization Metadata of KIT Data Manager are stored in relational databases using well defined schemas and can be accessed using Java or REST service APIs.
For content metadata there is no fixed API or schema available as content metadata strongly differs depending on the community and supported use cases. Rather there are basic workflows
available that can be implemented in order to extract, register and retrieve content metadata. How access to this metadata is realized depends on the system(s) in which the metadata
are registered. More details on enhance metadata handling can be found in <<ChapterEnhancedMetadataArchitecture,the according chapter>>.

For harvesting metadata directly KIT Data Manager provides an OAI-PMH interface available at `http://localhost:8080/KITDM/oaipmh` by default. Harvestable are all Digital Objects readable by the user with the userid 'OAI-PMH'
and all Digital Objects readable by the user group with the groupid 'WORLD'. 

Furthermore, it is possible to publish a Digital Object by granting access to userid 'WORLD' or groupid 'WORLD'. By doing to, everybody can access the Digital Object's landing page via `http://localhost:8080/KITDM?landing&oid=<OBJECT_ID>`
A sample landing page for object 3b1243b2-df09-4a98-ad87-21b7cda74be9 may look as follows: 

[[LandingPage]]
.The landing page for a public Digital Object. It allows to read/download metadata in different formats and to download public data associated with the Digital Object. 
image::screenshots/Landing.png[LandingPage, width="500px", align="center"]

If a Digital Object is not published, the user is requested to login in order to be able to see the landing page, which is of course only possible if the user is authorized to access the Digital Object.

[NOTE]
It is recommended to create a view named 'public' for storing published data of a Digital Object. If this view does not exists, publishing a Digital Objects will result in full access to the 'default' Data Organization view, which
might not be wanted in every case.

[[ChapterStaging]]
Staging
~~~~~~~
The term `Staging` basically stands for the process of transferring data in or out KIT Data Manager, either to access data manually or automatically for computing purposes.
As KIT Data Manager aims to be able to build up repository systems for large-scale scientific experiment data, the data transfer needs a special focus. 
In contrast to traditional repository systems KIT Data Manager provides reasonable throughput in order to be able to cope with the huge amounts of data 
delivered by scientists. In addition, scientific data can be very diverse, e.g. hundred of thousands of small files vs. a handful of files in the order of terabyte.
Therefore, the process of Staging in case of an ingest (for downloads this process is carried out the other way around) is divided into two parts: 

Caching::
   Caching is the plain data transfer to a temporary storage system which is accessible in an optimal way depending on the requirement, e.g. throughput, security or 
   geographical location. To achieve best results transfers to the cache are carried out using native clients or custom, optimized transfer tools. The location where the 
   cached data can be stored is provided and set up by KIT Data Manager using pre-configured `StagingAccessPoints`. A `StagingAccessPoint` defines the protocol as well as 
   the local and remote base path/URL accessible by the repository system and the user. Details about StagingAccessPoints can be found in the `Programming KIT Data Manager` 
   or `Administration UI` chapters.
Archiving::
   During archiving the data from the cache is validated as far as possible, metadata might be extracted and transfer post-processing may take place. Afterwards, the data
   is copied from the cache to a managed storage area where the repository system is taking care of the data. As soon as the data is in the managed storage, it can be expected 
   to be safe. Local copies and cached data can be removed and repository workflows start taking care of the data.

[NOTE]
Authentification and authorization for data transfers to and from the cache is not covered by KIT Data Manager. This offers a huge level of flexibility and allows to customzie 
the data transfer to possible needs. However, it is still possible to use the same user database that is used to obtain KIT Data Manager UserIDs, e.g. an LDAP server. The only thing that
has to be ensured is that for data ingests the written data has to be accessible by KIT Data Manager and for downloads the data, written by KIT Data Manager, must be readable by the user 
who wants to access the data. Typically, this can be achieved by running KIT Data Manager as a privileged user or by handling access permission on a group level. 

As mentioned before, the transfer into the archive storage is much more than a simple copy operation. The process of initially providing data associated with a 
Digital Object is called `Ingest`. During Ingest, different steps like metadata extraction, checksumming or even processing steps might be performed. For download operations 
requested data is copied first from the archive to the cache and can then be downloaded by the user using an according StagingAccessPoint. Furthermore, this workflow can be also used to copy the data to an external 
location, e.g. for data processing. In order to be able to provide supplementary data (e.g. files containing metadata), a special folder structure was defined for each staging location:

.Staging Tree Structure
-----------------------------------------------
31/12/69  23:59         <DIR>    data       <1>
31/12/69  23:59         <DIR>    generated  <2>
31/12/69  23:59         <DIR>    settings   <3>
-----------------------------------------------
<1> Contains the user data, e.g. uploaded files or files for download.
<2> Contains generated data, e.g. extracted metadata or processing results
<3> Contains KIT Data Manager-related settings, e.g. credentials for third-party transfers.

<<Staging,The graphic>> below shows the general Staging workflow. At the beginning, the `Staging Service` is accessed using an appropriate client, which might be 
a commandline client, a Web portal or something comparable. In case of downloading data the Staging operation is scheduled and will data be made available asynchronously 
by the Staging Service as this preparation may take a while (e.g. when restoring data from tape). In case of an ingest the cache is prepared immediately. 
As soon as the preparation of the data transfer operation is finished, the data is accessible from the cache by a data consumer or can be transferred to the cache by a data producer. 
Both, cache and archive storage must be accessible by KIT Data Manager, at least one of them must be accessible (also) in a POSIX-like way. 

[[Staging]]
.Staging workflow for ingest operations with KIT Data Manager. After selecting the data (1) a Digital Object is registered (2) and a new ingest is scheduled. As soon as the transfer is prepared, the data can be transfered (3). Finally, the ingest is marked for finalization (4). During finalization the cached data is copied to the archive (5), the Data Organization is obtained and content metadata might be extracted automatically. Finally, extracted content metadata is made accessible e.g. by a search index (6).
image::Staging.png[Staging, width="600px", align="center"]

[[ChapterDataOrganization]]
Data Organization
~~~~~~~~~~~~~~~~~
The Data Organization is closely coupled with the Staging and holds information on how the data belonging to a Digital Object is organized and where it is located. 
In the most simple case, after ingesting a file tree the Data Organization reflects exactly the ingested tree including CollectionNodes representing folders and FileNodes representing the files.
This allows to restore the file tree in case of a download in the representation the user expects. In addition, there might be attributes linked to Data Organization nodes, e.g. size or mime type of a node.

In more sophisticated scenarios the Data Organization might be customized according to user needs. These customizations are called `Views`. Views can be useful, e.g. to group all files with the same type belonging 
to one Digital Object or to transform a Digital Object's data into another format but keep the Data Organization linked to the particular Digital Object.

[[DataOrganization]]
.The graphic shows exemplarily different views for the Data Organization of a Digital Object. On the left hand side the default view with all contained files is shown. The second view contains only the data files, the third view contains a compressed version of the default view. Finally, the view on the right hand side contains generated files with a preview of the images contained in the default view that can be mapped by their names to each other.
image::DataOrganization.png[DataOrganization, width="400px", align="center"]

Apart from the basic file-based use case it is also possible to provide a custom Data Organization tree during ingest. This allows to register Data Organization trees where single nodes may refer to data located elsewhere, e.g. in another repository or on a web server.
For standard workflows, e.g. for ingest and dowload, this enhanced scenario implies some differences: 

Ingest::
   Instead of ingesting a file tree, one or more JSON file(s) containing a defined structure is/are ingested. An according StagingProcessor implemented by the class `edu.kit.dama.staging.processor.impl.ReferenceTreeIngestProcessor` 
   has to be configured properly in beforehand. This StagingProcessor will parse and register the Data Organization information provided by the JSON file(s). For more details please refern to the link:https://github.com/kit-data-manager/base/tree/master/Samples[Samples module]
   where the example `CustomDataOrganizationIngest` shows how to ingest a custom Data Organization and describes what are requirements and rules.
Download::
   For download there are also multiple options. The easiest way for supporting transparent access to remotely stored data is to refer to data openly accessible via HTTP. In that case, the LFNs available in the Data Organization nodes can be 
   directly accessed by the user to obtain the data. Furthermore, the REST-based Data Download described in the following section fully supports streaming of data from HTTP LFNs. Finally, also the Staging Service allows to stage data
   accessible via HTTP the same way as it is done for locally available data. However, this requires a bit more configuration effort.

For more information on how to enable the support for ingesting custom Data Organization trees please refer to the section  <<ChapterCustomDataOrganization,Support for Ingest of Custom Data Organization Trees>>.

[[SectionRESTBasedDownload]]
REST-based Data Download
^^^^^^^^^^^^^^^^^^^^^^^^
For obtaining data represented in the Data Organization typically the Staging Service is used to transfer the data to a location accessible using one of the configured StagingAccessPoints. 
An alternative that can be used for smaller downloads and for direct access avoiding the asynchronous staging process the Data Organization REST service offers direct access to the content 
of single nodes. Typically, this feature is used the same way as any other REST endpoint and the access is authorized via OAuth or any other configure authentication mechanism. 
The URL for downloading the content of a Data Organization node is build as follows: 

`http://kitdm-host:8080/KITDM/rest/dataorganization/organization/download/{objectId}/{path}?groupId=USERS&viewName=default` 

The first part `http://kitdm-host:8080/KITDM/rest/dataorganization/organization/download/` is the base URL pointing to the download endpoint. Typically, only `kitdm-host` has to be changed according to the accessed
KIT DM instance. The second part of the URL defines the accessed digital object by its numeric `baseId` followed by the Data Organization path. The value of path can be one of the following: 

[cols="m,n", options="header"]
|============================================================================================================================
|Value of {path}|Delivered Content
|'nothing'|The entire content of the provided view zipped in a file named <DIGITAL_OBJECT_ID>.zip
|MyCollection/|Assuming that MyCollection refers to a collection node, the zipped content of the node and all children is returned.
|MyCollection/MyFile.txt|Assuming that MyFile.txt refers to a file node, the file content is returned using the automatically determined mime type of the file.
|100|The content of the node with the provided nodeId. Depending on the node type the call is handled similar to one of the previous cases, e.g. 100 typically refers to the root node. Hence, providing 100 behaves identical to the example in table row 1.
|============================================================================================================================

The arguments provided in the URL are optional and define the group used to authorize the access to the associated Digital Object and the Data Organization view that will be accessed. By default these values are `USERS` and `default`.

As mentioned before, access to the REST-based data download is authorized e.g. via OAuth. However, there may be scenarios where one wants to provide open access to repository content, e.g. for showing an image, stored in the 
repository, on a web page. For this purpose the Data Organization offers the so called `authorization-less access`. The concept of this feature is to provide open access to a particular part of the Data Organization of a 
Digital Object without any user or group based authentification and authorization. Of course, this feature should be used with care in order to avoid opening data that should not be accessible by everybody.

Currently, there are four different kinds of `authorization-less access`:

[cols="m,n", options="header"]
|============================================================================================================================
|Type|Description
|Data Organization View|Allows to provide one or more Data Organization views that are publicly accessible. List of Data Organization views that are publicly accessible. Attention: You should never define the `default` view publicly available using this feature as this would grant access to the data of all Digital Objects.
|Data Organization Attribute| Allows to define an attribute that, if assigned to a Data Organization node, allows public access. The advantage of this approach is its fine-grained applicability on single collection nodes or file nodes also allowing to public single files of a Digital Object.
|Collection Node|For testing only! This option grants access to all collection nodes in all Digital Objects. It should never be enabled in production environments.
|File Node Filter|Allows to provide a regular expression granting public access to all file nodes in all Digital Objects and all Data Organization views for which the node name fulfills the regular expression.  
|============================================================================================================================

For setting up `authorization-less access` for the Data Organization service please refer to chapter <<ChapterAuthorizationLessAccess,Authorization-less Access to Data Organization>>.

[[ChapterDataWorkflow]]
Data Workflows
~~~~~~~~~~~~~~

A very special feature that distinguishes KIT Data Manager from other research data repository systems is the ability to trigger data processing workflows. This allows to execute data workflows seamlessly integrated
into repository system workflows. The repository system takes care of transferring the data to the processing environment, monitoring the execution and ingesting the results back to the repository system
and link them to the input object(s). Furthermore, single processing tasks can be chained to construct complex data processing workflows. The Data Workflow module is based on three major entities:

Execution Environment::
   The Execution Environment is the physical environment where a single data workflow task is executed. It might be the local machine or a compute cluster. The access to an execution environment is 
   implemented using an appropriate `ExecutionEnvironmentHandler` taking care of the preparation of the application and the input data, the execution of the application and its monitoring 
   as well as the ingest of the results, the creation of links to the input Digital Objects and the cleanup. However, this process can be abstracted quite well so that different handler implementations
   only have to take care about the actual execution/submission of the application and the monitoring of its status. The data transfer can be fully covered in a generic way using the Staging Service of 
   KIT Data Manager by assigning a StagingAccessPoint to each execution environment.
Data Workflow Task Configuration:: 
   A Data Workflow Task Configuration describes a single task that can be executed alone or after a predecessor task to chain multiple tasks. An actual instance of a Data Workflow Task Configuration is
   just called Data Workflow task. A task configuration consists of basic metadata, e.g. name, version, description and keywords, a package URL, which is pointing to a ZIP file containing everything 
   needed to execute the task's application, e.g. libraries, executables and a wrapper script named 'run.sh', and fixed application arguments. For all tasks registered in a KIT Data Manager instance 
   the combination of name, version, application package URL and application arguments is unique. If any of these fields changes, a new task version or an entirely new task must be created. 
   This is mandatory in order to be able to collect reliable provenance information later on. As all tasks are executed by an according ExecutionEnvironmentHandler fully automatically, the successful 
   execution of the task in the targeted environment should be tested in beforehand before registering the task in the repository system. This excludes principle errors, e.g. missing dependencies.
   Due to this required effort (packaging and testing the application) Data Workflow Tasks are mainly interesting for tasks executed many times.
Environment Property::
   Finally, there are Environment Properties allowing to describe capabilities of an execution environment, e.g. the platform or existing libraries, and the requirements of a Data Workflow Task. 
   In both cases, environment properties can be chosen from a common pool of properties and before registering a task execution in an execution environment capabilities and requirements are 
   matched to determine whether an execution environment can principally handle a task. However, there is currently no mechanism to ensure that an execution environment is really
   providing a specific environment property, e.g. the defined platform or a software package.

After describing the major entities the question is, how they work together. The first point here is the actual application that will be executed as a Data Workflow Task. Such application must be packed 
in a Zip archive with the following structure:

.Application Package Structure 
-----------------------------------------------
31/12/69  23:59         <DIR>    app        <1>
31/12/69  23:59         123      run.sh     <2>
-----------------------------------------------
<1> Contains the user application, e.g. libraries, binaries and static configuration files.
<2> The execution wrapper script for setting up and calling the user application. 

The actual user application may or may not be located in a directory called `app` but it is recommended to achieve a clean cut between data workflow and user application. The execution wrapper consistes of 
two parts: a common part that is recommended for all wrapper scripts and a specific part for setting up and calling the actual user application. The base script looks as follows:

[source,sh]
--------------------------------------
#!/bin/sh

#Variable definition for accessing/storing data. The placeholder variables, e.g. ${data.input.dir}, are replaced by the workflow service using the 
#appropriate values for the according task execution and execution environment. 
export INPUT_DIR=${data.input.dir}
export OUTPUT_DIR=${data.output.dir}
export WORKING_DIR=${working.dir}
export TEMP_DIR=${temp.dir}

#Place environment checks here, if necessary, to allow debugging. However, if something is missing at this point, the process will fail 
#either way.

#Now, the execution of the user application starts. The variables above should be provided to the process in a proper way depending on 
#the kind of the process, e.g. for Java processes via -DINPUT_DIR=$INPUT_DIR 

#At this point, the user application is executed, e.g. via './app/MainBinary $@'. The argument $@ is recommended to forward all fixed command line arguments 
#provided by the task configuration and the dynamic arguments optionally provided for each specific task instance. Each user application should return
#a proper exit code (0 for success, any other value for error).

#Obtain the exit code of the process, print out a logging message for debugging, and exit the wrapper script using the exit code of the internal
#process to allow a proper handling by the workflow service. If the exit code is 0 the data ingest of all data stored in $OUTPUT_DIR is triggered.
#Otherwise, the task will remain in an error state and needs user interaction.
EXIT=$?
echo "Execution finished."
exit $EXIT
--------------------------------------
 
It is recommended to use this wrapper script as it allows to obtain the proper directories from the repository and to provide an exit code for proper callback. 
The sample script above shows one feature allowing the repository to provide information for the user application: the variable substitution. All variables are 
pointing to an absolute path within the execution environment. Available variables are:

[cols="m,n", options="header"]
|============================================================================================================================
|Variable|Content
|${data.input.dir}|The directory containing all staged input data. If multiple Digital Objects are input for one task, the data of all object will be located in this directory.
|${data.output.dir}|The directory that can be used to store output data. All data located in this directory will be ingested as a new Digital Object as soon as the processing has succeeded. 
|${working.dir}|The working directory where the application archive is extracted to. Furthermore, the execution settings, which can be provided for each task execution, 
are stored in a file `dataworkflow.properties`, which is also located in the working directory.
|${temp.dir}|A temporary directory where the user application can store intermediate data. The content of this directory will be removed during task cleanup.
|============================================================================================================================

By default, the Data Workflow Service replaces these variables only in the file `run.sh`. In order to enforce variable substitution in custom files, e.g. settings or other application-specific files,
an empty file named `dataworkflow_substitution` has to be placed in each directory where substitution should take place. Variables substitution will then be applied to all files in this 
directory with a size smaller than 10 MB.

The actual execution of a workflow task is covered by an associated execution environment handler. This handler executes each task in multiple phases which are the following: 

[cols="m,n,o", options="header"]
|============================================================================================================================
|Phase Name|Actions|Next Phase
|SCHEDULED|Initial phase after creation.|PREPARING
|PREPARING|Creation of task directories, obtaining and extracting application package and performing variable substitution.|STAGING
|STAGING|Provide the data of all input Digital Objects in the input directory of the task.|PROCESSING
|PROCESSING|Use the concrete handler implementation to execute/submit and monitor the application execution.|INGEST
|INGEST|Ingest the data located in the task output directory as a new Digital Object.|CLEANUP
|CLEANUP|Remove all task directories and their contents.|-
|============================================================================================================================

Each phase has a PHASE_SUCCESSFUL and a PHASE_FAILED state. If a phase has been completed successfully in during the last handler execution cycle it will enter the next phase in the next cycle. If the 
phase execution has failed, the task has to be reset to the last phase's SUCCESSFUL state manually in order to reattempt the phase execution. The actual execution of tasks is triggered either via a Cron job
executing the `DataWorkflowTrigger` script of by creating an appropriate job schedule using the <<ChapterJobScheduling,AdminUI>>. 

[NOTE]
For each handled workflow task only one (or no) state transition will occur during a single Cron/job schedule execution. Possible transitions are depicted in figure <<FigureTaskStatus,TaskStatus>>. In case of data transfer
tasks, e.g. Staging and Ingest, the phase might be entered multiple times as long as the data transfer operation has not finished, yet. All other phases are implemented in a synchronous way so the according Cron/job schedule
execution won't finish until the phase is either in its SUCCESSFUL or FAILED state.

[[FigureTaskStatus]]
.Task state transitions of data workflow tasks.
image::TaskStatus.png[TaskStatus]

Summarizing, the Data Workflow Service offer a great potential for processing Digital Objects in an automated way including the tracking of provenance information for better reproducability. For more details
on how to setup execution environments, workflow tasks and triggering them, please refer to the chapters <<ChapterInstallation,Installation>> of KIT Data Manager and <<ChapterSettings,Settings>> 
of the Administration UI.

[[ChapterAudit]]
Audit
~~~~~
The Audit Service provides functionalities to capture audit information about changes of resources stored in the repository system. One example for using audit information is the documentation of the lifecycle of a Digital Object, 
starting with the creation and ingestion of content, modification of metdata, the validation of content and/or metadata and the migration or replication. Finally, the audit information may also contain deaccession and deletion of the resource.

In KIT Data Manager there are two ways how audit events are triggered: internally and externally. Internal audit events are statically integrated into KIT Data Manager workflows in order to ensure that they are triggered as soon as they occur,
e.g. the successful creation of a Digital Object via REST endpoint. For performance reasons internal events are only triggered in REST endpoints. If a Digital Object is created directly using the MetadataManagement Java API, an according
audit event has to be triggered manually. Currently, internal audit events are generated if a Study, an Investigation or a Digital Object are created and modified. Other audit events might be added in future. 
For more information on how to to this please refer to the the according sample in the link:https://github.com/kit-data-manager/base/tree/master/Samples[Samples module] of KIT Data Manager.

The setup of the Audit component is shown in figure <<FigureAuditWorkflow,AuditWorkflow>>.

[[FigureAuditWorkflow]]
.Workflow of publishing and consuming audit events. An event is produced by an EventProducer, e.g. by a REST service during the creation of a digital object. The event is published using the configured EventPublisher. Currently, the default EventPublisher is realized by RabbitMQ. For the configured EventPublisher there should be an according EventReceiver, in our case there is a RabbitMQ receiver configured as servlet running in the local Tomcat container. The EventReceiver is then responsible to distribute received events to configured consumers, in the example there is a consumer writing all events to a logfile and another consumer writing the same events to a database.
image::audit-workflow.png[AuditWorkflow, width="600px", align="center"]

The reason why the current implementation of the Audit workflow is implemented using RabbitMQ is that RabbitMQ allows an asynchronous publishing of messages from different sources and takes care of persisting undelivered messages on its own. 
Therefore, RabbitMQ is expected to scale very well and the influence of publishing audit events should be very low. 

[NOTE]
By default, audit events are distributed via a locally running RabbitMQ server. If no server is installed or not properly configured audit events are logged via logback to the configured logfile (e.g. $CATALINA_HOME/temp//datamanager.log).